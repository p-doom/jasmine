{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e005359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from array_record.python.array_record_module import ArrayRecordReader\n",
    "import multiprocessing\n",
    "from tqdm import tqdm  # Using tqdm for a nice progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def hash_byte_data(bytes_data: bytes) -> str:\n",
    "    \"\"\"Calculates the SHA256 hash of a byte string.\"\"\"\n",
    "    return hashlib.sha256(bytes_data).hexdigest()\n",
    "\n",
    "\n",
    "def hash_numpy_frame(frame: np.ndarray) -> str:\n",
    "    \"\"\"Calculates the SHA256 hash of a numpy array.\"\"\"\n",
    "    return hashlib.sha256(np.ascontiguousarray(frame)).hexdigest()\n",
    "\n",
    "\n",
    "def get_episode_level_hash(video_path: str) -> tuple[str, str] | None:\n",
    "    \"\"\"\n",
    "    Reads a single array_record file, extracts the video, hashes it,\n",
    "    and returns the (hash, video_path) tuple.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = ArrayRecordReader(video_path)\n",
    "        record_data = reader.read()\n",
    "        record_unpickled = pickle.loads(record_data)\n",
    "        video_bytes = record_unpickled[\"raw_video\"]\n",
    "        video_hash = hash_byte_data(video_bytes)\n",
    "        return video_hash, video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_frame_level_hashes(video_path: str) -> tuple[list[str], str] | None:\n",
    "    \"\"\"\n",
    "    Reads a single array_record file, extracts the frames, hashes each frame,\n",
    "    and returns the (list of hashes, video_path) tuple.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = ArrayRecordReader(video_path)\n",
    "        record_data = pickle.loads(reader.read())\n",
    "\n",
    "        # video shape (seq_len, 64, 64, 3)\n",
    "        video_shape = (record_data[\"sequence_length\"], 64, 64, 3)\n",
    "        episode_tensor = np.frombuffer(record_data[\"raw_video\"], dtype=np.uint8)\n",
    "        episode_tensor = episode_tensor.reshape(video_shape)\n",
    "\n",
    "        frame_hashes = [hash_numpy_frame(frame) for frame in episode_tensor]\n",
    "\n",
    "        return frame_hashes, video_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_array_record_files(dir):\n",
    "    return [\n",
    "        os.path.join(dir, x) for x in os.listdir(dir) if x.endswith(\".array_record\")\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base data directory\n",
    "base = \"data/coinrun_episodes\"\n",
    "\n",
    "train_dir = os.path.join(base, \"train\")\n",
    "test_dir = os.path.join(base, \"test\")\n",
    "val_dir = os.path.join(base, \"val\")\n",
    "\n",
    "train_array_record_files = get_array_record_files(train_dir)\n",
    "test_array_record_files = get_array_record_files(test_dir)\n",
    "val_array_record_files = get_array_record_files(val_dir)\n",
    "\n",
    "array_record_files = (\n",
    "    train_array_record_files + test_array_record_files + val_array_record_files\n",
    ")\n",
    "print(f\"Found {len(array_record_files)} files to process.\")\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(f\"Using {num_processes} worker processes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find episode level duplicates\n",
    "duplicate_episode = defaultdict(list)\n",
    "\n",
    "# The 'with' statement ensures the pool is properly closed\n",
    "with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    # Use pool.imap_unordered for efficiency. It processes items as they are submitted\n",
    "    # and returns results as they complete, which is perfect for progress bars.\n",
    "    # We wrap the result iterator with tqdm to show progress.\n",
    "    results = pool.imap_unordered(get_episode_level_hash, array_record_files)\n",
    "\n",
    "    print(\"Processing files and calculating hashes...\")\n",
    "    for result in tqdm(results, total=len(array_record_files)):\n",
    "        if result:  # Ensure the worker didn't return None due to an error\n",
    "            video_hash, video_path = result\n",
    "            duplicate_episode[video_hash].append(video_path)\n",
    "\n",
    "print(\"\\nAggregation complete. Finding duplicates...\")\n",
    "duplicates = {h: paths for h, paths in duplicate_episode.items() if len(paths) > 1}\n",
    "\n",
    "print(f\"\\nFound {len(duplicates)} sets of duplicate videos.\")\n",
    "if duplicates:\n",
    "    # Print the first 5 duplicate sets as an example\n",
    "    for i, (h, paths) in enumerate(duplicates.items()):\n",
    "        print(f\"  - Hash: {h[:10]}... ({len(paths)} files)\")\n",
    "        for path in paths:\n",
    "            print(f\"    - {os.path.basename(path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame level duplicates\n",
    "# This dictionary will map a frame hash to a list of (video_path, frame_index)\n",
    "frame_dup_dict = defaultdict(list)\n",
    "\n",
    "with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    results = pool.imap_unordered(get_frame_level_hashes, array_record_files)\n",
    "\n",
    "    print(\"Processing files and calculating frame hashes...\")\n",
    "    for result in tqdm(results, total=len(array_record_files)):\n",
    "        if result:\n",
    "            frame_hashes, video_path = result\n",
    "            for frame_idx, frame_hash in enumerate(frame_hashes):\n",
    "                frame_dup_dict[frame_hash].append((video_path, frame_idx))\n",
    "\n",
    "print(\"\\nAggregation complete. Finding duplicate frames...\")\n",
    "duplicate_frames = {\n",
    "    hash: location for hash, location in frame_dup_dict.items() if len(location) > 1\n",
    "}\n",
    "\n",
    "\n",
    "total_frames = sum(len(locations) for locations in frame_dup_dict.values())\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "num_duplicate_frames = len(duplicate_frames.keys())\n",
    "print(f\"Number of duplicate frames: {num_duplicate_frames}\")\n",
    "percentage = num_duplicate_frames / total_frames\n",
    "print(f\"Percentage of duplicate frames: {percentage:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
